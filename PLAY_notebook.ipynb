{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gym[box2d] in /home/andrei/.local/lib/python3.10/site-packages (0.26.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/andrei/.local/lib/python3.10/site-packages (from gym[box2d]) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/andrei/.local/lib/python3.10/site-packages (from gym[box2d]) (1.23.4)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/andrei/.local/lib/python3.10/site-packages (from gym[box2d]) (0.0.8)\n",
      "Collecting box2d-py==2.3.5\n",
      "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pygame==2.1.0\n",
      "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting swig==4.*\n",
      "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
      "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[16 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Using setuptools (version 59.6.0).\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/Box2D\n",
      "  \u001b[31m   \u001b[0m copying library/Box2D/Box2D.py -> build/lib.linux-x86_64-3.10/Box2D\n",
      "  \u001b[31m   \u001b[0m copying library/Box2D/__init__.py -> build/lib.linux-x86_64-3.10/Box2D\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/Box2D/b2\n",
      "  \u001b[31m   \u001b[0m copying library/Box2D/b2/__init__.py -> build/lib.linux-x86_64-3.10/Box2D/b2\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'Box2D._Box2D' extension\n",
      "  \u001b[31m   \u001b[0m swigging Box2D/Box2D.i to Box2D/Box2D_wrap.cpp\n",
      "  \u001b[31m   \u001b[0m swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D/Box2D_wrap.cpp Box2D/Box2D.i\n",
      "  \u001b[31m   \u001b[0m error: command 'swig' failed: No such file or directory\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
      "Failed to build box2d-py\n",
      "Installing collected packages: swig, box2d-py, pygame\n",
      "  Running setup.py install for box2d-py ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[33m  DEPRECATION: box2d-py was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed box2d-py-2.3.5 pygame-2.1.0 swig-4.1.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install gym[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example basic usage from here:https://stable-baselines3.readthedocs.io/en/master/guide/examples.html\n",
    "\n",
    "\n",
    "import gym\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrei/.local/lib/python3.10/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"LunarLander-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'LunarLander-v2'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m DQN(\u001b[39m\"\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m\"\u001b[39m, env \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mLunarLander-v2\u001b[39m\u001b[39m\"\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39m# Train the agent and display a progress bar\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, progress_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Save the agent\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mdqn_lunar\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/dqn/dqn.py:275\u001b[0m, in \u001b[0;36mDQN.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    262\u001b[0m     \u001b[39mself\u001b[39m: DQNSelf,\n\u001b[1;32m    263\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DQNSelf:\n\u001b[0;32m--> 275\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    276\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    277\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    278\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    279\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[1;32m    280\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[1;32m    281\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[1;32m    282\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    283\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[1;32m    284\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    285\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    286\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:341\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    328\u001b[0m     \u001b[39mself\u001b[39m: OffPolicyAlgorithmSelf,\n\u001b[1;32m    329\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    339\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OffPolicyAlgorithmSelf:\n\u001b[0;32m--> 341\u001b[0m     total_timesteps, callback \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_learn(\n\u001b[1;32m    342\u001b[0m         total_timesteps,\n\u001b[1;32m    343\u001b[0m         eval_env,\n\u001b[1;32m    344\u001b[0m         callback,\n\u001b[1;32m    345\u001b[0m         eval_freq,\n\u001b[1;32m    346\u001b[0m         n_eval_episodes,\n\u001b[1;32m    347\u001b[0m         eval_log_path,\n\u001b[1;32m    348\u001b[0m         reset_num_timesteps,\n\u001b[1;32m    349\u001b[0m         tb_log_name,\n\u001b[1;32m    350\u001b[0m         progress_bar,\n\u001b[1;32m    351\u001b[0m     )\n\u001b[1;32m    353\u001b[0m     callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    355\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:315\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._setup_learn\u001b[0;34m(self, total_timesteps, eval_env, callback, eval_freq, n_eval_episodes, log_path, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    312\u001b[0m     pos \u001b[39m=\u001b[39m (replay_buffer\u001b[39m.\u001b[39mpos \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m replay_buffer\u001b[39m.\u001b[39mbuffer_size\n\u001b[1;32m    313\u001b[0m     replay_buffer\u001b[39m.\u001b[39mdones[pos] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_setup_learn(\n\u001b[1;32m    316\u001b[0m     total_timesteps,\n\u001b[1;32m    317\u001b[0m     eval_env,\n\u001b[1;32m    318\u001b[0m     callback,\n\u001b[1;32m    319\u001b[0m     eval_freq,\n\u001b[1;32m    320\u001b[0m     n_eval_episodes,\n\u001b[1;32m    321\u001b[0m     log_path,\n\u001b[1;32m    322\u001b[0m     reset_num_timesteps,\n\u001b[1;32m    323\u001b[0m     tb_log_name,\n\u001b[1;32m    324\u001b[0m     progress_bar,\n\u001b[1;32m    325\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:489\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[0;34m(self, total_timesteps, eval_env, callback, eval_freq, n_eval_episodes, log_path, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39m# Avoid resetting the environment when calling ``.learn()`` consecutive times\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m reset_num_timesteps \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_obs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mreset()  \u001b[39m# pytype: disable=annotation-type-mismatch\u001b[39;00m\n\u001b[1;32m    490\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_episode_starts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mnum_envs,), dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n\u001b[1;32m    491\u001b[0m     \u001b[39m# Retrieve unnormalized observation for saving into the buffer\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:64\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m     63\u001b[0m     obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs[env_idx]\u001b[39m.\u001b[39mreset()\n\u001b[0;32m---> 64\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_obs(env_idx, obs)\n\u001b[1;32m     65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obs_from_buf()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:94\u001b[0m, in \u001b[0;36mDummyVecEnv._save_obs\u001b[0;34m(self, env_idx, obs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys:\n\u001b[1;32m     93\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuf_obs[key][env_idx] \u001b[39m=\u001b[39m obs\n\u001b[1;32m     95\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_obs[key][env_idx] \u001b[39m=\u001b[39m obs[key]\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 1."
     ]
    }
   ],
   "source": [
    "# Instantiate the agent\n",
    "model = DQN(\"MlpPolicy\", env = \"LunarLander-v2\", verbose=1)\n",
    "# Train the agent and display a progress bar\n",
    "model.learn(total_timesteps=3, progress_bar=True)\n",
    "# Save the agent\n",
    "model.save(\"dqn_lunar\")\n",
    "del model  # delete trained model to demonstrate loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained agent\n",
    "# NOTE: if you have loading issue, you can pass `print_system_info=True`\n",
    "# to compare the system on which the model was trained vs the current one\n",
    "# model = DQN.load(\"dqn_lunar\", env=env, print_system_info=True)\n",
    "model = DQN.load(\"dqn_lunar\", env=env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agent\n",
    "# NOTE: If you use wrappers with your environment that modify rewards,\n",
    "#       this will be reflected here. To evaluate with original rewards,\n",
    "#       wrap environment in a \"Monitor\" wrapper before other wrappers.\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enjoy trained agent\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
